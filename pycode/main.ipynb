{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|code-fold: true\n",
    "#|output: false\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import platform\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ToPILImage, RandomHorizontalFlip, Resize\n",
    "\n",
    "from step_by_step import StepByStep\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have accumulated labels in the `data\\all_labels` folder. We need to:\n",
    "- load all of them into NCWH tensors\n",
    "- split them into train and valid (test labels will be separate), \n",
    "- create temporary Datasets and normalizer\n",
    "- create real Datasets and DataLoaders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a `x_tensor/y_tensor` from all_labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path('.').resolve().parent\n",
    "data_dir = proj_dir / 'data'\n",
    "train_imgs_dir = data_dir / 'all_labels/type_1/imgs'\n",
    "train_masks_dir = data_dir / 'all_labels/type_1/masks'\n",
    "image_paths = sorted(list(train_imgs_dir.glob('*.png')))\n",
    "mask_paths = sorted(list(train_masks_dir.glob('*.png')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([x.name for x in image_paths])\n",
    "b = set([x.name for x in mask_paths])\n",
    "assert len(a ^ b) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to go from a path to tensor use `torchvision.transforms.ToTensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor = ToTensor()(Image.open(image_paths[0]))\n",
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stack many images, use `torch.stack` (not the most memory efficient but it's ok):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorizer = ToTensor()\n",
    "x_tensor = []\n",
    "y_tensor = []\n",
    "\n",
    "for image_path, mask_path in zip(image_paths, mask_paths):\n",
    "    # load an image into a tensor and store in x_tensor, hopefully not too big:\n",
    "    image_tensor = tensorizer(Image.open(image_path))\n",
    "    mask_tensor = tensorizer(Image.open(mask_path).convert('RGB'))\n",
    "    \n",
    "    x_tensor.append(image_tensor)\n",
    "    y_tensor.append(mask_tensor)\n",
    "\n",
    "x_tensor = torch.stack(x_tensor)\n",
    "y_tensor = torch.stack(y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 3, 256, 256])\n",
      "torch.Size([162, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor.shape)\n",
    "print(y_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `torch.utils.data.random_split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 101, 76, 11, 44, 97, 18, 51, 86, 123, 125, 59, 0, 63, 92, 111, 114, 41, 95, 27, 67, 36, 110, 83, 62, 10, 127, 144, 69, 145, 143, 133, 117, 55, 58, 42, 89, 7, 94, 50, 38, 150, 70, 153, 137, 105, 155, 96, 1, 102, 138, 140, 93, 131, 4, 5, 141, 71, 21, 91, 35, 149, 124, 30, 151, 147, 85, 16, 160, 57, 32, 103, 115, 156, 74, 104, 77, 87, 6, 129, 80, 139, 60, 119, 75, 90, 61, 78, 46, 98, 134, 116, 128, 25, 154, 148, 108, 2, 15, 24, 79, 84, 135, 126, 107, 120, 37, 82, 52, 100, 68, 53, 54, 19, 118, 9, 132, 40, 31, 29, 146, 73, 161, 72, 152, 136, 48, 49, 8]\n",
      "[99, 66, 157, 121, 12, 64, 142, 130, 3, 14, 106, 33, 23, 65, 112, 88, 39, 45, 56, 13, 122, 47, 159, 81, 17, 28, 20, 34, 113, 43, 158, 26, 109]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(13)  # Important for consistency\n",
    "N = len(x_tensor)\n",
    "n_train = int(.8*N)\n",
    "n_val = N - n_train\n",
    "train_subset, val_subset = random_split(x_tensor, [n_train, n_val])\n",
    "\n",
    "train_idx = train_subset.indices\n",
    "val_idx = val_subset.indices\n",
    "\n",
    "print(train_idx)\n",
    "print(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = x_tensor[train_idx]\n",
    "y_train_tensor = y_tensor[train_idx]\n",
    "\n",
    "x_val_tensor = x_tensor[val_idx]\n",
    "y_val_tensor = y_tensor[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 3, 256, 256])\n",
      "torch.Size([33, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor.shape)\n",
    "print(x_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our very simple dataset with transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedTensorDataset(Dataset):\n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create **temporary** `Dataset` to extract normalization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=tensor([0.1902, 0.2077, 0.1599]), std=tensor([0.1060, 0.1060, 0.1071]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor)\n",
    "temp_loader = DataLoader(temp_dataset, batch_size=32)\n",
    "normalizer = StepByStep.make_normalizer(temp_loader)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Datasets and Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create **real** `Datasets` and `DataLoaders`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_composer = Compose([normalizer])  # train_composer will have augmentations later\n",
    "val_composer = Compose([normalizer])\n",
    "\n",
    "train_dataset = TransformedTensorDataset(x_train_tensor, y_train_tensor, transform=train_composer)\n",
    "val_dataset = TransformedTensorDataset(x_val_tensor, y_val_tensor, transform=val_composer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define model, optimizer, and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = nn.Sequential()\n",
    "model_logistic.add_module('flatten', nn.Flatten())\n",
    "model_logistic.add_module('output', nn.Linear(25, 1, bias=True))\n",
    "model_logistic.add_module('sigmoid', nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('output.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0078,  0.0652, -0.1866, -0.1366, -0.0493,  0.0314,  0.0079, -0.1319,\n",
       "           -0.1687, -0.1602,  0.1702,  0.0634,  0.0076, -0.1402,  0.1080,  0.1755,\n",
       "            0.0316, -0.0058, -0.0456,  0.0117, -0.1726,  0.1206,  0.1004,  0.0990,\n",
       "            0.1439]], requires_grad=True)),\n",
       " ('output.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.1549], requires_grad=True))]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_logistic.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(block_input, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Padding of ((output_size-1) * stride - input_size + kernel_size) // 2 \n",
    "    is supposed to ensure output_size = ceil(input_size/stride) \n",
    "    (from https://stackoverflow.com/questions/48491728/what-is-the-behavior-of-same-padding-when-stride-is-greater-than-1)\n",
    "    but I can't make it work. For now padding padding = (kernel_size - 1) // 2 does the trick. \n",
    "    \"\"\"\n",
    "#     input_size = block_input.shape[2]\n",
    "#     output_size = int(np.ceil(input_size / stride))\n",
    "#     padding = int(np.ceil(((output_size-1) * stride - input_size + kernel_size) // 2))\n",
    "\n",
    "    padding = (kernel_size - 1) // 2\n",
    "    return padding\n",
    "\n",
    "\n",
    "def make_segnet_residual_block(block_input, depth, kernel_size, stride):\n",
    "    # main branch\n",
    "    x = nn.Conv2d(block_input.shape[1], depth, kernel_size, stride, \n",
    "                  get_padding(block_input, kernel_size, stride))(block_input)\n",
    "    x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "    x = nn.ELU()(x)\n",
    "    x = nn.Conv2d(x.shape[1], depth, kernel_size, stride, \n",
    "                 get_padding(x, kernel_size, stride))(x)\n",
    "    x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "\n",
    "    # residual branch\n",
    "    branch = nn.Conv2d(block_input.shape[1], depth, 1, stride,\n",
    "                      get_padding(block_input, 1, stride))(block_input)  # kernel_size=1 here so padding=0\n",
    "    branch = nn.BatchNorm2d(branch.shape[1])(branch)\n",
    "\n",
    "    x = torch.cat([x, branch], 1)\n",
    "    x = nn.ELU()(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "class Segnet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=2):\n",
    "        super(Segnet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(n_channels, 32, 5, 2, get_padding(x, 5, 2))\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(32)\n",
    "        self.elu_1 = nn.ELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.elu_1(x)\n",
    "        div2 = make_segnet_residual_block(x, 32, 3, 1)\n",
    "\n",
    "        x = nn.Conv2d(x.shape[1], 64, 5, 2, get_padding(x, 5, 2))(x)\n",
    "        x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "        x = nn.ELU()(x)\n",
    "        div4 = make_segnet_residual_block(x, 64, 3, 1)\n",
    "    \n",
    "        x = nn.Conv2d(x.shape[1], 128, 5, 2, get_padding(x, 5, 2))(x)\n",
    "        x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "        x = nn.ELU()(x)\n",
    "        div8 = make_segnet_residual_block(x, 128, 3, 1)\n",
    "    \n",
    "        x = nn.ConvTranspose2d(x.shape[1], 64, 5, 2, get_padding(x, 5, 2))(x)\n",
    "        x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "    \n",
    "        added = torch.cat([x, div4], 1)\n",
    "        x = nn.ELU()(added)\n",
    "    \n",
    "        x = nn.ConvTranspose2d(x.shape[1], 32, 5, 2, get_padding(x, 5, 2))(x)\n",
    "        x = nn.BatchNorm2d(x.shape[1])(x)\n",
    "    \n",
    "        added = torch.cat([x, div2], 1)\n",
    "        x = nn.ELU()(added)\n",
    "    \n",
    "        x = nn.ConvTranspose2d(x.shape[1], self.n_classes, 5, 2, get_padding(x, 5, 2))(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segnet()"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1])\n",
    "b = torch.Tensor([2])\n",
    "c = torch.cat([a,b], dim=0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1])\n",
    "b = torch.Tensor([2])\n",
    "c = torch.cat([a,b], dim=0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.]])\n",
      "tensor([[1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1])\n",
    "b = torch.Tensor([2])\n",
    "c = torch.stack([a,b], dim=0)\n",
    "d = torch.stack([a,b], dim=1)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "from .unet_parts import *\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar_panel",
   "language": "python",
   "name": "solar_panel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
